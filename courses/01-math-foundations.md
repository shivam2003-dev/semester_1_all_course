---
layout: course
title: "Math Foundations for AI/ML"
short_description: "Essential mathematical concepts including linear algebra, calculus, probability and statistics"
description: "Master the mathematical fundamentals required for AI and Machine Learning"
credits: 4
level: "Beginner to Intermediate"
instructor: "Faculty"
topics:
  - Linear Algebra Basics
  - Vectors and Matrices
  - Calculus Fundamentals
  - Probability Theory
  - Statistical Methods
  - Optimization
  - Principal Component Analysis (PCA)
  - SVM Optimization (Primal/Dual)
github_repo: "https://github.com/shivam2003-dev/semester_1_all_course"
---

## üìù Course Overview

This course provides the mathematical foundation necessary for understanding machine learning algorithms and AI concepts. Whether you're new to mathematics or need a refresher, this course builds your skills systematically.

---

## üìö Course Content

### Module 1: Linear Algebra Basics

<div class="alert alert-note">
  <h4>üìå Note: Linear Algebra is Crucial</h4>
  <p>Linear algebra is the backbone of machine learning. Understanding vectors, matrices, and their operations is essential for grasping how neural networks and most ML algorithms work.</p>
</div>

#### Key Topics:
- **Vectors and Vector Spaces**: Understanding n-dimensional spaces
- **Matrices**: Operations, inverse, determinant
- **Eigenvalues and Eigenvectors**: Critical for PCA and other techniques
- **Matrix Decomposition**: LU, QR, SVD decompositions

#### Important Concepts:

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Matrix Dimensions</h4>
  <p>Always check matrix dimensions before operations. For matrix multiplication A√óB, the number of columns in A must equal the number of rows in B. This is a common source of errors!</p>
</div>

**Key Formulas:**
```
Matrix Multiplication (A: m√ón, B: n√óp ‚Üí C: m√óp)
C[i,j] = Œ£ A[i,k] √ó B[k,j]

Eigenvalue Equation:
Av = Œªv (where v is eigenvector, Œª is eigenvalue)

Determinant (2√ó2):
|A| = ad - bc for matrix [[a,b],[c,d]]
```

<div class="alert alert-tip">
  <h4>üí° Industry Tip: GPU Computing</h4>
  <p>In industry, linear algebra operations are heavily optimized on GPUs. Libraries like CUDA and cuBLAS can make calculations 100x faster. Learn PyTorch or TensorFlow for practical implementation.</p>
</div>

---

### Module 2: Calculus Fundamentals

#### Key Topics:
- **Derivatives**: Understanding rates of change
- **Partial Derivatives**: Multivariable calculus
- **Chain Rule**: Essential for backpropagation
- **Gradient Descent**: Optimization algorithm
- **Integration**: Area under curves, probability

<div class="alert alert-note">
  <h4>üìå Note: Calculus and Neural Networks</h4>
  <p>The chain rule in calculus is the foundation of backpropagation in neural networks. Understanding how derivatives compose is critical for deep learning.</p>
</div>

**Essential Derivatives:**
```
d/dx(x^n) = n√óx^(n-1)
d/dx(e^x) = e^x
d/dx(ln(x)) = 1/x
d/dx(sin(x)) = cos(x)

Chain Rule:
d/dx[f(g(x))] = f'(g(x)) √ó g'(x)

Gradient Vector:
‚àáf = [‚àÇf/‚àÇx‚ÇÅ, ‚àÇf/‚àÇx‚ÇÇ, ..., ‚àÇf/‚àÇx‚Çô]
```

<div class="alert alert-danger">
  <h4>üî¥ Common Mistake: Chain Rule</h4>
  <p>Students often forget to multiply by the derivative of the inner function. Always apply the chain rule correctly in nested functions.</p>
</div>

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Gradient Descent</h4>
  <p>Exam questions often ask about gradient descent convergence. Remember: learning rate matters! Too high = divergence, too low = slow convergence. Be able to explain this with an example.</p>
</div>

---

### Module 3: Probability Theory

#### Key Topics:
- **Probability Basics**: Sample spaces, events
- **Conditional Probability**: Bayes' theorem
- **Random Variables**: Discrete and continuous
- **Probability Distributions**: Normal, Binomial, Poisson
- **Expectation and Variance**: Statistical measures

<div class="alert alert-info">
  <h4>‚ÑπÔ∏è Important: Bayes' Theorem</h4>
  <p>Bayes' theorem is fundamental to many ML algorithms. It describes the relationship between conditional probabilities and is the basis for Bayesian inference.</p>
</div>

**Key Formulas:**
```
Probability: P(A) = Number of favorable outcomes / Total outcomes

Bayes' Theorem:
P(A|B) = P(B|A) √ó P(A) / P(B)

Expectation: E[X] = Œ£ x √ó P(x)

Variance: Var(X) = E[X¬≤] - (E[X])¬≤

Normal Distribution:
f(x) = (1/(œÉ‚àö(2œÄ))) √ó e^(-(x-Œº)¬≤/(2œÉ¬≤))
```

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Independence Assumption</h4>
  <p>Many algorithms assume feature independence, which is rarely true in practice. This can lead to suboptimal model performance if ignored.</p>
</div>

---

### Module 4: Statistical Methods

#### Key Topics:
- **Descriptive Statistics**: Mean, median, variance, standard deviation
- **Hypothesis Testing**: p-values, significance levels
- **Correlation and Covariance**: Relationships between variables
- **Distributions**: Understanding different types
- **Sampling**: Population vs. sample

<div class="alert alert-note">
  <h4>üìå Note: Correlation vs Causation</h4>
  <p>A fundamental principle in statistics: Correlation does not imply causation. Two variables can be correlated without one causing the other.</p>
</div>

**Key Statistical Measures:**
```
Mean: Œº = Œ£x / n

Standard Deviation: œÉ = ‚àö(Œ£(x-Œº)¬≤ / n)

Covariance: Cov(X,Y) = E[(X-Œº‚Çì)(Y-Œº·µß)]

Correlation: œÅ = Cov(X,Y) / (œÉ‚Çì √ó œÉ·µß)

Z-Score: z = (x - Œº) / œÉ
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Normal Distribution</h4>
  <p>Know the 68-95-99.7 rule: 68% of data within 1œÉ, 95% within 2œÉ, 99.7% within 3œÉ. This is frequently tested.</p>
</div>

---

### Module 5: Optimization

#### Key Topics:
- **Gradient Descent**: First-order optimization
- **Stochastic Gradient Descent (SGD)**: Practical version
- **Convergence**: When to stop optimization
- **Learning Rates**: Hyperparameter selection
- **Momentum and Acceleration**: Advanced techniques

<div class="alert alert-tip">
  <h4>üí° Industry Tip: Optimization in Practice</h4>
  <p>In production systems, companies use advanced optimizers like Adam, RMSprop, or AdamW. These adapt the learning rate per parameter, often outperforming simple SGD. Study these implementations!</p>
</div>

**Gradient Descent Update:**
```
Œ∏_new = Œ∏_old - Œ± √ó ‚àáJ(Œ∏)

where Œ± is the learning rate and ‚àáJ is the gradient
```

<div class="alert alert-danger">
  <h4>üî¥ Common Issue: Vanishing/Exploding Gradients</h4>
  <p>Deep networks can suffer from gradients that become too small or too large. Solutions include careful initialization, batch normalization, and appropriate activation functions.</p>
</div>

---

### Module 6: Dimensionality Reduction & PCA

#### Key Topics:
- **Variance Maximization**: Projecting data to directions of maximum variance
- **Covariance Matrix & Eigen Decomposition**: Link to principal components
- **Explained Variance Ratio**: Selecting number of components
- **Whitening & Reconstruction**: Transformations and inverse mapping

**Core Equations:**
```
Given centered data matrix X ‚àà ‚Ñù^{n√ód}

Covariance: Œ£ = (1/n) X·µÄX

Eigen Decomposition: Œ£ v·µ¢ = Œª·µ¢ v·µ¢
Principal Components: columns of V = [v‚ÇÅ, v‚ÇÇ, ..., v_d]
Project to k components: Z = X V_k  (V_k: top-k eigenvectors)

Explained Variance Ratio (EVR):
EVR_k = (Œ£_{i=1..k} Œª·µ¢) / (Œ£_{i=1..d} Œª·µ¢)
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Choosing k</h4>
  <p>Plot the scree curve (eigenvalues) and pick k at the elbow. Alternatively, choose the smallest k with EVR ‚â• 0.95 for strong compression.</p>
</div>

<div class="alert alert-tip">
  <h4>üí° Industry Tip: PCA for Pipelines</h4>
  <p>Use PCA to reduce dimensionality before clustering or regression to stabilize models and speed up training. Standardize features before PCA.</p>
</div>

---

### Module 7: Optimization for Support Vector Machines (SVM)

#### Key Topics:
- **Primal Formulation**: Margin maximization with hinge loss
- **Dual Formulation**: Lagrange multipliers and kernels
- **KKT Conditions**: Complementary slackness and optimality
- **Kernel Trick**: Implicit feature mapping via kernels

**Primal (Soft-Margin) SVM:**
```
Given training set {(x·µ¢, y·µ¢)} with y·µ¢ ‚àà {‚àí1, +1}

min_{w,b,Œæ}  (1/2)‚Äñw‚Äñ¬≤ + C Œ£ Œæ·µ¢
subject to: y·µ¢ (w·µÄ x·µ¢ + b) ‚â• 1 ‚àí Œæ·µ¢,  Œæ·µ¢ ‚â• 0
```

**Dual Form:**
```
max_Œ±  Œ£ Œ±·µ¢ ‚àí (1/2) Œ£Œ£ Œ±·µ¢ Œ±‚±º y·µ¢ y‚±º K(x·µ¢, x‚±º)
subject to: 0 ‚â§ Œ±·µ¢ ‚â§ C,  Œ£ Œ±·µ¢ y·µ¢ = 0

Decision function: f(x) = sign(Œ£ Œ±·µ¢ y·µ¢ K(x·µ¢, x) + b)
```

**KKT Conditions (at optimum):**
```
Œ±·µ¢ ‚â• 0, Œæ·µ¢ ‚â• 0
Œ±·µ¢ [ y·µ¢ (w·µÄ x·µ¢ + b) ‚àí 1 + Œæ·µ¢ ] = 0
Œº·µ¢ Œæ·µ¢ = 0,  Œº·µ¢ ‚â• 0  (for slack constraints)
```

<div class="alert alert-info">
  <h4>‚ÑπÔ∏è Important: Support Vectors</h4>
  <p>Only points with Œ±·µ¢ > 0 contribute to the decision boundary. These are the support vectors; removing non-support vectors doesn‚Äôt change the classifier.</p>
</div>

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Feature Scaling</h4>
  <p>SVMs are sensitive to feature scales. Always standardize features before training to avoid dominance of high-variance features.</p>
</div>

---

## üéØ Learning Outcomes

By the end of this course, you should be able to:

- ‚úÖ Perform matrix operations and understand eigenvalues/eigenvectors
- ‚úÖ Apply calculus concepts to optimization problems
- ‚úÖ Calculate probabilities and apply Bayes' theorem
- ‚úÖ Interpret statistical results and hypothesis tests
- ‚úÖ Implement gradient descent optimization
- ‚úÖ Perform PCA and reason about explained variance
- ‚úÖ Derive SVM primal/dual and apply KKT conditions

---

## üî¨ Practice Problems

<div class="alert alert-info">
  <h4>Practice Question 1: Matrix Operations</h4>
  <p>Given matrices A (2√ó3) and B (3√ó4), what are the dimensions of AB? Can you calculate BA?</p>
</div>

<div class="alert alert-info">
  <h4>Practice Question 2: Bayes' Theorem</h4>
  <p>A disease affects 1% of population. A test is 99% accurate. If you test positive, what's the probability you actually have the disease?</p>
</div>

<div class="alert alert-info">
  <h4>Practice Question 3: Gradient Descent</h4>
  <p>Explain why a learning rate that's too high might cause gradient descent to diverge.</p>
</div>

---

## ‚ö° Exam Tips

<div class="card">
  <h4>üéì What to Focus On</h4>
  <ul>
    <li>Master the <strong>chain rule</strong> - it appears in many problems</li>
    <li>Know how to compute <strong>eigenvalues and eigenvectors</strong></li>
    <li>Understand <strong>Bayes' theorem</strong> conceptually and mathematically</li>
    <li>Be able to <strong>solve optimization problems</strong> using calculus</li>
    <li>Know the properties of common distributions (normal, exponential)</li>
  </ul>
</div>

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Common Exam Mistakes</h4>
  <ul>
    <li>Forgetting to multiply by chain rule derivatives</li>
    <li>Incorrect matrix dimension calculation</li>
    <li>Confusing correlation with causation in statistics</li>
    <li>Sign errors in gradient descent updates</li>
  </ul>
</div>

---

## üíº Industry Applications

### Data Science
Probability and statistics are core to A/B testing, user segmentation, and model evaluation.

### Machine Learning
Linear algebra and calculus are essential for training neural networks and implementing algorithms.

### Finance & Economics
Optimization and probability theory drive portfolio management and risk assessment.

### Computer Vision
Linear algebra is used extensively for image transformations and deep learning models.

---

## üîó External Resources

### Recommended Reading
- [Linear Algebra - MIT OpenCourseWare](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/)
- [3Blue1Brown: Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- [3Blue1Brown: Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr28mDVmKKX8p-yc_LKiU)

### Tools & Libraries
- **NumPy**: Python library for numerical computing
- **SciPy**: Scientific computing with optimization tools
- **SymPy**: Symbolic mathematics

### Research Papers
- [Linear Algebra and Learning from Data - Gilbert Strang](https://math.mit.edu/~gs/learningfromdata/)

---

## üìã Quick Reference Cheatsheet

### Linear Algebra
```
Vector dot product: a¬∑b = Œ£ a·µ¢b·µ¢
Matrix transpose: (AB)·µÄ = B·µÄA·µÄ
Trace: tr(A) = Œ£ a·µ¢·µ¢ (sum of diagonal)
```

### Calculus
```
Product rule: (fg)' = f'g + fg'
Quotient rule: (f/g)' = (f'g - fg') / g¬≤
```

### Probability
```
P(A‚à™B) = P(A) + P(B) - P(A‚à©B)
P(A‚à©B) = P(A|B) √ó P(B)
```

### Statistics
```
Confidence Interval: Œº ¬± z √ó (œÉ/‚àön)
t-statistic: t = (xÃÑ - Œº) / (s/‚àön)
```

---

## üìû Need Help?

- Check the [Resources page]({{ site.baseurl }}/resources/) for additional learning materials
- Review the practice problems and solutions
- Consult the external resource links provided above

**Last Updated**: December 2025
