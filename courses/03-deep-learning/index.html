<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Neural Networks - AIML Course Hub</title>
  <meta name="description" content="Master Deep Learning concepts and advanced neural network architectures">
  <link rel="stylesheet" href="/semester_1_all_course/assets/css/style.css">
  <link rel="icon" type="image/png" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='75' font-size='75'>ğŸ“š</text></svg>">
</head>
<body>
  <header>
  <div class="container">
    <div class="site-title">ğŸ“š AIML Course Hub</div>
    <div class="site-description">Comprehensive AI/ML Course Website with Notes, Resources & Industry Tips</div>
  </div>
</header>

  
  <main class="content-wrapper">
    <div class="container">
      <div style="display: grid; grid-template-columns: 280px 1fr; gap: 2em; align-items: start;">
        <aside class="global-sidebar">
  <nav class="global-nav">
    <ul>
      <li><a href="/semester_1_all_course/">ğŸ  Home</a></li>
      <li><a href="/semester_1_all_course/courses/">ğŸ“š Courses</a></li>
      <li><a href="/semester_1_all_course/resources/">ğŸ”— Resources</a></li>
      <li><a href="/semester_1_all_course/about/">â„¹ï¸ About</a></li>
    </ul>
  </nav>

  
  

  
  <div class="sidebar-section">
    <h3>ğŸ“– Course Info</h3>
    <p><strong>Course:</strong> Deep Neural Networks</p>
    <p><strong>Credits:</strong> 4</p>
    <p><strong>Instructor:</strong> Faculty</p>
    <p><strong>Level:</strong> Advanced</p>
  </div>

  
  <div class="sidebar-section">
    <h3>ğŸ“Š Topics</h3>
    <ul>
      
      <li>Artificial Neural Networks</li>
      
      <li>Backpropagation Algorithm</li>
      
      <li>Convolutional Neural Networks</li>
      
      <li>Recurrent Neural Networks</li>
      
      <li>Attention Mechanisms</li>
      
      <li>Transformers</li>
      
    </ul>
  </div>
  

  
  
  
  

  
  <div class="sidebar-section">
    <h3>ğŸ§  DNN Modules</h3>
    <ol class="module-list">
      
      <li><a href="/semester_1_all_course/courses/deep-learning/01-foundations/">Foundations of Deep Learning</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/02-perceptron/">Artificial Neuron and Perceptron</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/03-linear-nets/">Linear Neural Networks</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/04-mlp/">Deep Feedforward Neural Networks (MLP)</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/05-cnn/">Convolutional Neural Networks (CNNs)</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/06-rnn/">Recurrent Neural Networks (RNNs)</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/07-attention/">Attention Mechanisms</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/08-transformers/">Transformers</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/09-optimization/">Optimization of Deep Models</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/10-regularization/">Regularization and Generalization</a></li>
      
      <li><a href="/semester_1_all_course/courses/deep-learning/11-labs/">Hands-On & Experiential Learning</a></li>
      
    </ol>
  </div>
  

  
  <p style="margin-top: 1.5em;"><a href="https://github.com/shivam2003-dev/semester_1_all_course" class="btn" style="width: 100%; text-align: center; display: block;">ğŸ™ GitHub Repository</a></p>
  
  
</aside>

        <div>
          <article class="course-detail">
  <div class="breadcrumb">
    <li><a href="/semester_1_all_course/">Home</a></li>
    <li><a href="/semester_1_all_course/courses/">Courses</a></li>
    <li>Deep Neural Networks</li>
  </div>

  <div class="course-header" style="background: linear-gradient(135deg, #2c3e50, #3498db); color: white; padding: 2em; border-radius: 8px; margin-bottom: 2em;">
    <h1 style="color: white; border: none; padding-bottom: 0; margin-bottom: 0.5em;">Deep Neural Networks</h1>
    <p style="margin: 0; opacity: 0.9; font-size: 1.1em;">Master Deep Learning concepts and advanced neural network architectures</p>
  </div>

  <h2 id="-course-overview">ğŸ“ Course Overview</h2>

<p>This course explores deep neural networks - the foundation of modern AI. From basic neural networks to cutting-edge transformers, youâ€™ll learn architectures that power state-of-the-art applications in vision, language, and more.</p>

<hr />

<h2 id="-course-content">ğŸ“š Course Content</h2>

<h3 id="module-1-fundamentals-of-neural-networks">Module 1: Fundamentals of Neural Networks</h3>

<h4 id="key-topics">Key Topics:</h4>
<ul>
  <li><strong>Perceptron</strong>: Single neuron, activation functions</li>
  <li><strong>Multilayer Perceptron (MLP)</strong>: Deep networks, hidden layers</li>
  <li><strong>Activation Functions</strong>: ReLU, Sigmoid, Tanh, Softmax</li>
  <li><strong>Loss Functions</strong>: Cross-entropy, MSE, custom losses</li>
  <li><strong>Initialization</strong>: Xavier, He initialization</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Universal Approximation</h4>
  <p>Theoretically, a neural network with a single hidden layer can approximate any continuous function. However, deep networks learn more efficiently with fewer parameters.</p>
</div>

<p><strong>Basic Neural Network Equations:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Forward Pass (Single Layer):
z = Wx + b
a = activation(z)

Backward Pass (Gradient Computation):
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚z Ã— âˆ‚z/âˆ‚W

ReLU Activation:
ReLU(x) = max(0, x)

Softmax (Multi-class):
softmax(záµ¢) = e^záµ¢ / Î£ e^zâ±¼
</code></pre></div></div>

<div class="alert alert-warning">
  <h4>âš ï¸ Warning: Dying ReLU</h4>
  <p>ReLU neurons can die (always output 0) if learning rate is too high. Use techniques like Leaky ReLU or proper initialization to prevent this.</p>
</div>

<div class="alert alert-tip">
  <h4>ğŸ’¡ Industry Tip: Activation Functions Matter</h4>
  <p>Choice of activation function significantly impacts training. ReLU dominates modern networks due to computational efficiency, but Leaky ReLU and GELU are becoming popular. Experiment in your domain.</p>
</div>

<hr />

<h3 id="module-2-backpropagation--training">Module 2: Backpropagation &amp; Training</h3>

<h4 id="key-topics-1">Key Topics:</h4>
<ul>
  <li><strong>Backpropagation Algorithm</strong>: Computing gradients</li>
  <li><strong>Gradient Descent Variants</strong>: SGD, Momentum, Adam</li>
  <li><strong>Batch Normalization</strong>: Stabilizing training</li>
  <li><strong>Dropout</strong>: Regularization technique</li>
  <li><strong>Early Stopping</strong>: Preventing overfitting</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Backpropagation is Chain Rule</h4>
  <p>Backpropagation is simply the chain rule applied to neural networks. Understanding this concept is crucial for debugging and improving deep learning models.</p>
</div>

<p><strong>Backpropagation Through Layers:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For layer l:
Î´Ë¡ = (WË¡âºÂ¹)áµ€ Î´Ë¡âºÂ¹ âŠ™ Ïƒ'(zË¡)  [âŠ™ is element-wise multiplication]

Weight Update:
W := W - Î± Ã— âˆ‚L/âˆ‚W

Batch Normalization:
y = Î³ Ã— (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ) + Î²
</code></pre></div></div>

<div class="alert alert-success">
  <h4>âœ… Exam Tip: Optimizers</h4>
  <p>Know the differences: SGD is simple but can be slow, Momentum accelerates SGD, Adam adapts learning rates per parameter. Adam is most popular but SGD with momentum often generalizes better.</p>
</div>

<p><strong>Common Optimizers Comparison:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SGD: Î¸ := Î¸ - Î± Ã— âˆ‡L
Momentum: v := Î²v + (1-Î²)âˆ‡L; Î¸ := Î¸ - Î± Ã— v
Adam: Combines momentum + adaptive learning rates
</code></pre></div></div>

<div class="alert alert-warning">
  <h4>âš ï¸ Warning: Batch Size Matters</h4>
  <p>Batch size affects gradient estimates and generalization. Large batches = fast training but may hurt generalization. Typical ranges: 32-512. Experiment for your dataset!</p>
</div>

<div class="alert alert-danger">
  <h4>ğŸ”´ Common Mistake: Normalization</h4>
  <p>Batch normalization should be applied BEFORE activation functions for best results. The order: Linear â†’ BatchNorm â†’ Activation</p>
</div>

<hr />

<h3 id="module-3-convolutional-neural-networks-cnns">Module 3: Convolutional Neural Networks (CNNs)</h3>

<h4 id="key-topics-2">Key Topics:</h4>
<ul>
  <li><strong>Convolution Operation</strong>: Filters, kernels, stride</li>
  <li><strong>Pooling</strong>: Max-pooling, average pooling</li>
  <li><strong>Popular Architectures</strong>: LeNet, AlexNet, VGG, ResNet</li>
  <li><strong>Transfer Learning</strong>: Using pre-trained models</li>
  <li><strong>Object Detection</strong>: YOLO, R-CNN variants</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Convolution is Correlation</h4>
  <p>In neural networks, "convolution" is actually cross-correlation (no flipping of the kernel). This is a mathematical simplification that works well in practice.</p>
</div>

<p><strong>Convolution Dimensions:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output Size = (Input - Kernel + 2Ã—Padding) / Stride + 1

Example: Input 32Ã—32, Kernel 3Ã—3, Padding 1, Stride 1
Output = (32 - 3 + 2) / 1 + 1 = 32Ã—32

Parameter Count = Kernel_h Ã— Kernel_w Ã— In_channels Ã— Out_channels
</code></pre></div></div>

<div class="alert alert-success">
  <h4>âœ… Exam Tip: Receptive Field</h4>
  <p>Understanding receptive field is crucial. It shows what area of input each output pixel "sees". Deeper layers have larger receptive fields, capturing larger contexts.</p>
</div>

<p><strong>Common CNN Architectures:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LeNet-5: Early architecture, simple (handwriting)
AlexNet: Won ImageNet 2012, introduced ReLU
VGG: Simple, uses 3Ã—3 kernels extensively
ResNet: Introduced skip connections, enables very deep networks
EfficientNet: Balances accuracy and efficiency
</code></pre></div></div>

<div class="alert alert-warning">
  <h4>âš ï¸ Warning: Transfer Learning Pitfalls</h4>
  <p>When using pre-trained models, be careful with learning rates. Start with lower learning rates when fine-tuning. Sometimes freezing early layers helps if you have little data.</p>
</div>

<div class="alert alert-tip">
  <h4>ğŸ’¡ Industry Tip: Practical CNNs</h4>
  <p>In production, use efficient architectures like MobileNet, EfficientNet, or SqueezeNet for mobile/edge devices. ResNet is robust for server-side applications. Always consider latency requirements.</p>
</div>

<hr />

<h3 id="module-4-recurrent-neural-networks-rnns">Module 4: Recurrent Neural Networks (RNNs)</h3>

<h4 id="key-topics-3">Key Topics:</h4>
<ul>
  <li><strong>RNN Fundamentals</strong>: Recurrent connections, sequential processing</li>
  <li><strong>LSTM (Long Short-Term Memory)</strong>: Addressing vanishing gradients</li>
  <li><strong>GRU (Gated Recurrent Unit)</strong>: Simplified LSTM</li>
  <li><strong>Bidirectional RNNs</strong>: Forward and backward processing</li>
  <li><strong>Sequence-to-Sequence Models</strong>: Machine translation, summarization</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Vanishing/Exploding Gradients in RNNs</h4>
  <p>RNNs suffer more from vanishing gradients because gradients are multiplied across time steps. LSTMs use gates and cell states to address this. This is why LSTMs are preferred over vanilla RNNs.</p>
</div>

<p><strong>LSTM Cell Equations:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Forget Gate: fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bf)
Input Gate: iâ‚œ = Ïƒ(Wáµ¢Â·[hâ‚œâ‚‹â‚, xâ‚œ] + báµ¢)
Cell Candidate: CÌƒâ‚œ = tanh(WcÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bc)
Cell State: Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
Output Gate: oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bo)
Hidden State: hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
</code></pre></div></div>

<div class="alert alert-danger">
  <h4>ğŸ”´ Common Mistake: Sequence Padding</h4>
  <p>Always mask padding tokens when computing loss! Otherwise, the model learns to predict random values for padding, wasting capacity.</p>
</div>

<p><strong>RNN Architectures:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>One-to-One: Single input to single output (standard)
One-to-Many: Image captioning (image â†’ words)
Many-to-One: Sentiment analysis (words â†’ label)
Many-to-Many: Machine translation, NER (sequence â†’ sequence)
</code></pre></div></div>

<div class="alert alert-success">
  <h4>âœ… Exam Tip: LSTM vs GRU</h4>
  <p>LSTMs have 3 gates (forget, input, output), GRUs have 2 gates (reset, update). GRUs are simpler and faster, LSTMs are more expressive. Choose based on data size and computational constraints.</p>
</div>

<div class="alert
 alert-tip">
  <h4>ğŸ’¡ Industry Tip: Attention Over RNNs</h4>
  <p>Transformers with attention have largely replaced RNNs in production due to better parallelization and performance. However, RNNs are still used for streaming/online scenarios where you process data as it arrives.</p>
</div>

<hr />

<h3 id="module-5-attention-mechanisms--transformers">Module 5: Attention Mechanisms &amp; Transformers</h3>

<h4 id="key-topics-4">Key Topics:</h4>
<ul>
  <li><strong>Attention Mechanism</strong>: Query, Key, Value</li>
  <li><strong>Multi-Head Attention</strong>: Parallel attention heads</li>
  <li><strong>Transformer Architecture</strong>: Encoder-decoder model</li>
  <li><strong>Self-Attention</strong>: Capturing long-range dependencies</li>
  <li><strong>Applications</strong>: BERT, GPT, Vision Transformers</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Attention is a Game-Changer</h4>
  <p>Attention mechanisms allow the model to focus on relevant parts of input. This is more powerful than RNNs for capturing long-range dependencies and enables better parallelization.</p>
</div>

<p><strong>Attention Computation:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scaled Dot-Product Attention:
Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Ã— V

Multi-Head Attention:
head_i = Attention(QWáµ¢áµ , KWáµ¢á´·, VWáµ¢á´±)
MultiHead(Q,K,V) = Concat(headâ‚,...,headâ‚•)Wá´¼
</code></pre></div></div>

<div class="alert alert-success">
  <h4>âœ… Exam Tip: Positional Encoding</h4>
  <p>Transformers don't have inherent position information like RNNs. Positional encodings (sinusoidal or learned) add position information. Understand why this matters!</p>
</div>

<p><strong>Transformer Building Blocks:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Encoder:
- Multi-head self-attention
- Feed-forward network
- Layer normalization and residual connections

Decoder:
- Masked multi-head self-attention
- Cross-attention to encoder outputs
- Feed-forward network
</code></pre></div></div>

<div class="alert alert-warning">
  <h4>âš ï¸ Warning: Quadratic Complexity</h4>
  <p>Attention has O(nÂ²) complexity where n is sequence length. For very long sequences, this becomes prohibitive. Solutions include: sparse attention, linear attention variants, or chunking.</p>
</div>

<div class="alert alert-tip">
  <h4>ğŸ’¡ Industry Tip: Pre-trained Transformers</h4>
  <p>In 2025, using pre-trained models (BERT, GPT, LLaMA, etc.) is standard. Fine-tune on your task rather than training from scratch. Understand prompt engineering for LLMs!</p>
</div>

<hr />

<h3 id="module-6-advanced-topics">Module 6: Advanced Topics</h3>

<h4 id="key-topics-5">Key Topics:</h4>
<ul>
  <li><strong>Regularization</strong>: Dropout, L1/L2, weight decay</li>
  <li><strong>Data Augmentation</strong>: Improving generalization</li>
  <li><strong>Distributed Training</strong>: Multi-GPU, multi-node</li>
  <li><strong>Model Compression</strong>: Quantization, pruning, distillation</li>
  <li><strong>Explainability</strong>: Understanding model decisions</li>
</ul>

<div class="alert alert-note">
  <h4>ğŸ“Œ Note: Generalization is Key</h4>
  <p>Deep learning's strength is learning features from raw data. The challenge is making models generalize to unseen data. Regularization, data augmentation, and proper validation are critical.</p>
</div>

<p><strong>Regularization Techniques:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dropout: Randomly deactivate neurons during training (probability p)
Label Smoothing: Replace hard targets (one-hot) with soft targets
Early Stopping: Stop when validation loss plateaus
Data Augmentation: Random rotations, crops, colors, etc.
Weight Decay: L2 regularization on weights
</code></pre></div></div>

<div class="alert alert-success">
  <h4>âœ… Exam Tip: Choosing Regularization</h4>
  <p>Use dropout for large models, label smoothing for small models, data augmentation always. Combine techniques for best results. Monitor train vs validation loss to diagnose overfitting.</p>
</div>

<hr />

<h2 id="-learning-outcomes">ğŸ¯ Learning Outcomes</h2>

<p>By the end of this course, you should be able to:</p>

<ul>
  <li>âœ… Implement neural networks from scratch</li>
  <li>âœ… Train networks efficiently using modern optimizers</li>
  <li>âœ… Build and train CNNs for computer vision tasks</li>
  <li>âœ… Implement RNNs/LSTMs for sequence tasks</li>
  <li>âœ… Use transformers for state-of-the-art performance</li>
  <li>âœ… Debug training issues and optimize models</li>
  <li>âœ… Apply transfer learning and fine-tuning</li>
</ul>

<hr />

<h2 id="-exam-tips">âš¡ Exam Tips</h2>

<div class="alert alert-success">
  <h4>âœ… Key Concepts to Master</h4>
  <ul>
    <li><strong>Backpropagation</strong>: Chain rule through layers</li>
    <li><strong>Optimization</strong>: SGD, Momentum, Adam differences</li>
    <li><strong>CNN Dimensions</strong>: Calculate output sizes</li>
    <li><strong>LSTM Gates</strong>: Forget, Input, Output gates</li>
    <li><strong>Attention</strong>: Q, K, V computation</li>
    <li><strong>Regularization</strong>: When and why to use each technique</li>
  </ul>
</div>

<div class="alert alert-warning">
  <h4>âš ï¸ Common Exam Mistakes</h4>
  <ul>
    <li>Forgetting batch dimension in tensor operations</li>
    <li>Incorrect output size calculations for convolutions</li>
    <li>Not accounting for mask tokens in sequence models</li>
    <li>Confusing forward and backward passes</li>
    <li>Applying batch norm after activation (should be before)</li>
  </ul>
</div>

<hr />

<h2 id="-real-world-applications">ğŸ’¼ Real-World Applications</h2>

<h3 id="computer-vision">Computer Vision</h3>
<ul>
  <li>Image classification, object detection, segmentation</li>
  <li>Self-driving cars, medical imaging, face recognition</li>
</ul>

<h3 id="natural-language-processing">Natural Language Processing</h3>
<ul>
  <li>Machine translation, text generation, sentiment analysis</li>
  <li>Chatbots, information extraction, question answering</li>
</ul>

<h3 id="audio--speech">Audio &amp; Speech</h3>
<ul>
  <li>Speech recognition, music generation, speaker identification</li>
</ul>

<h3 id="recommendation-systems">Recommendation Systems</h3>
<ul>
  <li>User-item interactions, collaborative filtering with deep networks</li>
</ul>

<hr />

<h2 id="-external-resources">ğŸ”— External Resources</h2>

<h3 id="courses">Courses</h3>
<ul>
  <li><a href="https://course.fast.ai/">Fast.ai - Practical Deep Learning</a></li>
  <li><a href="http://cs231n.stanford.edu/">Stanford CS231N - CNN for Visual Recognition</a></li>
  <li><a href="https://web.stanford.edu/class/cs224n/">Stanford CS224N - NLP with RNNs</a></li>
</ul>

<h3 id="key-papers">Key Papers</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need - Transformer paper</a></li>
  <li><a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet Classification with Deep CNNs - AlexNet</a></li>
  <li><a href="https://www.bioinf.jku.at/publications/older/2604.pdf">LSTM - Hochreiter &amp; Schmidhuber</a></li>
</ul>

<h3 id="libraries">Libraries</h3>
<ul>
  <li><strong>PyTorch</strong>: Flexible, pythonic framework</li>
  <li><strong>TensorFlow/Keras</strong>: Production-ready, easier API</li>
  <li><strong>JAX</strong>: Modern, functional approach</li>
  <li><strong>Hugging Face Transformers</strong>: Pre-trained models for NLP</li>
</ul>

<h3 id="cheatsheets">Cheatsheets</h3>
<ul>
  <li><a href="https://pytorch.org/tutorials/">PyTorch Cheatsheet</a></li>
  <li><a href="https://keras.io/">Keras API Reference</a></li>
  <li><a href="https://github.com/vdumoulin/conv_arithmetic">CNN Architecture Visualization</a></li>
</ul>

<hr />

<h2 id="-need-help">ğŸ“ Need Help?</h2>

<ul>
  <li>Practice implementing architectures from scratch</li>
  <li>Experiment with pre-trained models on Hugging Face</li>
  <li>Review papers and implementation details</li>
  <li>Check the <a href="/semester_1_all_course/resources/">Resources page</a> for more materials</li>
</ul>

<p><strong>Last Updated</strong>: December 2025</p>

</article>

<style>
  @media (max-width: 768px) {
    .global-sidebar { display: none; }
    .container > div { grid-template-columns: 1fr !important; }
  }
</style>

        </div>
      </div>
    </div>
  </main>

  <footer>
  <div class="container">
    <div>
      <h3>ğŸ“š About This Site</h3>
      <p>A comprehensive course platform designed for AI/ML students featuring detailed notes, exam tips, industry insights, and curated resources.</p>
    </div>
    <div>
      <h3>ğŸ“– Quick Links</h3>
      <ul>
        <li><a href="/semester_1_all_course/">Home</a></li>
        <li><a href="/semester_1_all_course/courses/">Courses</a></li>
        <li><a href="/semester_1_all_course/resources/">Resources</a></li>
        <li><a href="/semester_1_all_course/about/">About</a></li>
      </ul>
    </div>
    <div>
      <h3>ğŸ”— External Resources</h3>
      <ul>
        <li><a href="https://www.coursera.org/" target="_blank">Coursera</a></li>
        <li><a href="https://www.edx.org/" target="_blank">edX</a></li>
        <li><a href="https://github.com/" target="_blank">GitHub</a></li>
        <li><a href="https://arxiv.org/" target="_blank">arXiv</a></li>
      </ul>
    </div>
    <div>
      <h3>ğŸ’¡ Repository</h3>
      <p><a href="https://github.com/shivam2003-dev/semester_1_all_course" target="_blank">View on GitHub</a></p>
      <p style="margin-top: 1em;"><a href="https://github.com/shivam2003-dev" target="_blank">@shivam2003-dev</a></p>
    </div>
  </div>
  <div class="footer-bottom">
    <p>&copy; 2025 AIML Course Hub. All rights reserved. | Built with Jekyll & GitHub Pages</p>
  </div>
</footer>

</body>
</html>
