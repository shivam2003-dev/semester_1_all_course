---
layout: course
collection: dnn
course: dnn
order: 7
module_title: "Attention Mechanisms"
short_description: "Queries, keys, values; self- and multi-head attention"
---

## Overview

- Queries, keys, values
- Dot-product and additive attention
- Self-attention and multi-head attention
- Positional encoding

## Equations

- Attention(Q,K,V) = softmax(QKᵀ / √dₖ) V

## Notes

- Positional encodings add order information to sequences

## Exam Tips

- Derive scaled dot-product attention and explain scaling

## Industry Tips

- Attention underpins modern NLP and vision transformers
