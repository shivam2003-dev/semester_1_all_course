---
layout: course
collection: dnn
course: dnn
order: 8
module_title: "Transformers"
short_description: "Encoder/decoder blocks, residuals, layer norm, variants"
---

## Overview

- Transformer architecture
- Encoder, decoder, and transformer blocks
- Residual connections and layer normalization
- Encoder-only, decoder-only, encoder–decoder models
- Vision Transformers (ViT) and large-scale pretraining

## Notes

- Complexity: O(n²) with sequence length; use sparse/linear variants for long sequences

## Exam Tips

- Explain why positional encoding is required

## Industry Tips

- Fine-tune pre-trained LLMs; prompt engineering and adapters reduce cost
