---
layout: course
title: "Deep Neural Networks"
short_description: "Neural network architectures, backpropagation, CNN, RNN, attention mechanisms and transformers"
description: "Master Deep Learning concepts and advanced neural network architectures"
credits: 4
level: "Advanced"
instructor: "Faculty"
topics:
  - Artificial Neural Networks
  - Backpropagation Algorithm
  - Convolutional Neural Networks
  - Recurrent Neural Networks
  - Attention Mechanisms
  - Transformers
github_repo: "https://github.com/shivam2003-dev/semester_1_all_course"
---

## üìù Course Overview

This course explores deep neural networks - the foundation of modern AI. From basic neural networks to cutting-edge transformers, you'll learn architectures that power state-of-the-art applications in vision, language, and more.

---

## üìö Course Content

### Module 1: Fundamentals of Neural Networks

#### Key Topics:
- **Perceptron**: Single neuron, activation functions
- **Multilayer Perceptron (MLP)**: Deep networks, hidden layers
- **Activation Functions**: ReLU, Sigmoid, Tanh, Softmax
- **Loss Functions**: Cross-entropy, MSE, custom losses
- **Initialization**: Xavier, He initialization

<div class="alert alert-note">
  <h4>üìå Note: Universal Approximation</h4>
  <p>Theoretically, a neural network with a single hidden layer can approximate any continuous function. However, deep networks learn more efficiently with fewer parameters.</p>
</div>

**Basic Neural Network Equations:**
```
Forward Pass (Single Layer):
z = Wx + b
a = activation(z)

Backward Pass (Gradient Computation):
‚àÇL/‚àÇW = ‚àÇL/‚àÇa √ó ‚àÇa/‚àÇz √ó ‚àÇz/‚àÇW

ReLU Activation:
ReLU(x) = max(0, x)

Softmax (Multi-class):
softmax(z·µ¢) = e^z·µ¢ / Œ£ e^z‚±º
```

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Dying ReLU</h4>
  <p>ReLU neurons can die (always output 0) if learning rate is too high. Use techniques like Leaky ReLU or proper initialization to prevent this.</p>
</div>

<div class="alert alert-tip">
  <h4>üí° Industry Tip: Activation Functions Matter</h4>
  <p>Choice of activation function significantly impacts training. ReLU dominates modern networks due to computational efficiency, but Leaky ReLU and GELU are becoming popular. Experiment in your domain.</p>
</div>

---

### Module 2: Backpropagation & Training

#### Key Topics:
- **Backpropagation Algorithm**: Computing gradients
- **Gradient Descent Variants**: SGD, Momentum, Adam
- **Batch Normalization**: Stabilizing training
- **Dropout**: Regularization technique
- **Early Stopping**: Preventing overfitting

<div class="alert alert-note">
  <h4>üìå Note: Backpropagation is Chain Rule</h4>
  <p>Backpropagation is simply the chain rule applied to neural networks. Understanding this concept is crucial for debugging and improving deep learning models.</p>
</div>

**Backpropagation Through Layers:**
```
For layer l:
Œ¥À° = (WÀ°‚Å∫¬π)·µÄ Œ¥À°‚Å∫¬π ‚äô œÉ'(zÀ°)  [‚äô is element-wise multiplication]

Weight Update:
W := W - Œ± √ó ‚àÇL/‚àÇW

Batch Normalization:
y = Œ≥ √ó (x - Œº_batch) / ‚àö(œÉ¬≤_batch + Œµ) + Œ≤
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Optimizers</h4>
  <p>Know the differences: SGD is simple but can be slow, Momentum accelerates SGD, Adam adapts learning rates per parameter. Adam is most popular but SGD with momentum often generalizes better.</p>
</div>

**Common Optimizers Comparison:**
```
SGD: Œ∏ := Œ∏ - Œ± √ó ‚àáL
Momentum: v := Œ≤v + (1-Œ≤)‚àáL; Œ∏ := Œ∏ - Œ± √ó v
Adam: Combines momentum + adaptive learning rates
```

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Batch Size Matters</h4>
  <p>Batch size affects gradient estimates and generalization. Large batches = fast training but may hurt generalization. Typical ranges: 32-512. Experiment for your dataset!</p>
</div>

<div class="alert alert-danger">
  <h4>üî¥ Common Mistake: Normalization</h4>
  <p>Batch normalization should be applied BEFORE activation functions for best results. The order: Linear ‚Üí BatchNorm ‚Üí Activation</p>
</div>

---

### Module 3: Convolutional Neural Networks (CNNs)

#### Key Topics:
- **Convolution Operation**: Filters, kernels, stride
- **Pooling**: Max-pooling, average pooling
- **Popular Architectures**: LeNet, AlexNet, VGG, ResNet
- **Transfer Learning**: Using pre-trained models
- **Object Detection**: YOLO, R-CNN variants

<div class="alert alert-note">
  <h4>üìå Note: Convolution is Correlation</h4>
  <p>In neural networks, "convolution" is actually cross-correlation (no flipping of the kernel). This is a mathematical simplification that works well in practice.</p>
</div>

**Convolution Dimensions:**
```
Output Size = (Input - Kernel + 2√óPadding) / Stride + 1

Example: Input 32√ó32, Kernel 3√ó3, Padding 1, Stride 1
Output = (32 - 3 + 2) / 1 + 1 = 32√ó32

Parameter Count = Kernel_h √ó Kernel_w √ó In_channels √ó Out_channels
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Receptive Field</h4>
  <p>Understanding receptive field is crucial. It shows what area of input each output pixel "sees". Deeper layers have larger receptive fields, capturing larger contexts.</p>
</div>

**Common CNN Architectures:**
```
LeNet-5: Early architecture, simple (handwriting)
AlexNet: Won ImageNet 2012, introduced ReLU
VGG: Simple, uses 3√ó3 kernels extensively
ResNet: Introduced skip connections, enables very deep networks
EfficientNet: Balances accuracy and efficiency
```

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Transfer Learning Pitfalls</h4>
  <p>When using pre-trained models, be careful with learning rates. Start with lower learning rates when fine-tuning. Sometimes freezing early layers helps if you have little data.</p>
</div>

<div class="alert alert-tip">
  <h4>üí° Industry Tip: Practical CNNs</h4>
  <p>In production, use efficient architectures like MobileNet, EfficientNet, or SqueezeNet for mobile/edge devices. ResNet is robust for server-side applications. Always consider latency requirements.</p>
</div>

---

### Module 4: Recurrent Neural Networks (RNNs)

#### Key Topics:
- **RNN Fundamentals**: Recurrent connections, sequential processing
- **LSTM (Long Short-Term Memory)**: Addressing vanishing gradients
- **GRU (Gated Recurrent Unit)**: Simplified LSTM
- **Bidirectional RNNs**: Forward and backward processing
- **Sequence-to-Sequence Models**: Machine translation, summarization

<div class="alert alert-note">
  <h4>üìå Note: Vanishing/Exploding Gradients in RNNs</h4>
  <p>RNNs suffer more from vanishing gradients because gradients are multiplied across time steps. LSTMs use gates and cell states to address this. This is why LSTMs are preferred over vanilla RNNs.</p>
</div>

**LSTM Cell Equations:**
```
Forget Gate: f‚Çú = œÉ(Wf¬∑[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bf)
Input Gate: i‚Çú = œÉ(W·µ¢¬∑[h‚Çú‚Çã‚ÇÅ, x‚Çú] + b·µ¢)
Cell Candidate: CÃÉ‚Çú = tanh(Wc¬∑[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bc)
Cell State: C‚Çú = f‚Çú ‚äô C‚Çú‚Çã‚ÇÅ + i‚Çú ‚äô CÃÉ‚Çú
Output Gate: o‚Çú = œÉ(Wo¬∑[h‚Çú‚Çã‚ÇÅ, x‚Çú] + bo)
Hidden State: h‚Çú = o‚Çú ‚äô tanh(C‚Çú)
```

<div class="alert alert-danger">
  <h4>üî¥ Common Mistake: Sequence Padding</h4>
  <p>Always mask padding tokens when computing loss! Otherwise, the model learns to predict random values for padding, wasting capacity.</p>
</div>

**RNN Architectures:**
```
One-to-One: Single input to single output (standard)
One-to-Many: Image captioning (image ‚Üí words)
Many-to-One: Sentiment analysis (words ‚Üí label)
Many-to-Many: Machine translation, NER (sequence ‚Üí sequence)
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: LSTM vs GRU</h4>
  <p>LSTMs have 3 gates (forget, input, output), GRUs have 2 gates (reset, update). GRUs are simpler and faster, LSTMs are more expressive. Choose based on data size and computational constraints.</p>
</div>

<div class="alert
 alert-tip">
  <h4>üí° Industry Tip: Attention Over RNNs</h4>
  <p>Transformers with attention have largely replaced RNNs in production due to better parallelization and performance. However, RNNs are still used for streaming/online scenarios where you process data as it arrives.</p>
</div>

---

### Module 5: Attention Mechanisms & Transformers

#### Key Topics:
- **Attention Mechanism**: Query, Key, Value
- **Multi-Head Attention**: Parallel attention heads
- **Transformer Architecture**: Encoder-decoder model
- **Self-Attention**: Capturing long-range dependencies
- **Applications**: BERT, GPT, Vision Transformers

<div class="alert alert-note">
  <h4>üìå Note: Attention is a Game-Changer</h4>
  <p>Attention mechanisms allow the model to focus on relevant parts of input. This is more powerful than RNNs for capturing long-range dependencies and enables better parallelization.</p>
</div>

**Attention Computation:**
```
Scaled Dot-Product Attention:
Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd‚Çñ) √ó V

Multi-Head Attention:
head_i = Attention(QW·µ¢·µ†, KW·µ¢·¥∑, VW·µ¢·¥±)
MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head‚Çï)W·¥º
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Positional Encoding</h4>
  <p>Transformers don't have inherent position information like RNNs. Positional encodings (sinusoidal or learned) add position information. Understand why this matters!</p>
</div>

**Transformer Building Blocks:**
```
Encoder:
- Multi-head self-attention
- Feed-forward network
- Layer normalization and residual connections

Decoder:
- Masked multi-head self-attention
- Cross-attention to encoder outputs
- Feed-forward network
```

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Warning: Quadratic Complexity</h4>
  <p>Attention has O(n¬≤) complexity where n is sequence length. For very long sequences, this becomes prohibitive. Solutions include: sparse attention, linear attention variants, or chunking.</p>
</div>

<div class="alert alert-tip">
  <h4>üí° Industry Tip: Pre-trained Transformers</h4>
  <p>In 2025, using pre-trained models (BERT, GPT, LLaMA, etc.) is standard. Fine-tune on your task rather than training from scratch. Understand prompt engineering for LLMs!</p>
</div>

---

### Module 6: Advanced Topics

#### Key Topics:
- **Regularization**: Dropout, L1/L2, weight decay
- **Data Augmentation**: Improving generalization
- **Distributed Training**: Multi-GPU, multi-node
- **Model Compression**: Quantization, pruning, distillation
- **Explainability**: Understanding model decisions

<div class="alert alert-note">
  <h4>üìå Note: Generalization is Key</h4>
  <p>Deep learning's strength is learning features from raw data. The challenge is making models generalize to unseen data. Regularization, data augmentation, and proper validation are critical.</p>
</div>

**Regularization Techniques:**
```
Dropout: Randomly deactivate neurons during training (probability p)
Label Smoothing: Replace hard targets (one-hot) with soft targets
Early Stopping: Stop when validation loss plateaus
Data Augmentation: Random rotations, crops, colors, etc.
Weight Decay: L2 regularization on weights
```

<div class="alert alert-success">
  <h4>‚úÖ Exam Tip: Choosing Regularization</h4>
  <p>Use dropout for large models, label smoothing for small models, data augmentation always. Combine techniques for best results. Monitor train vs validation loss to diagnose overfitting.</p>
</div>

---

## üéØ Learning Outcomes

By the end of this course, you should be able to:

- ‚úÖ Implement neural networks from scratch
- ‚úÖ Train networks efficiently using modern optimizers
- ‚úÖ Build and train CNNs for computer vision tasks
- ‚úÖ Implement RNNs/LSTMs for sequence tasks
- ‚úÖ Use transformers for state-of-the-art performance
- ‚úÖ Debug training issues and optimize models
- ‚úÖ Apply transfer learning and fine-tuning

---

## ‚ö° Exam Tips

<div class="alert alert-success">
  <h4>‚úÖ Key Concepts to Master</h4>
  <ul>
    <li><strong>Backpropagation</strong>: Chain rule through layers</li>
    <li><strong>Optimization</strong>: SGD, Momentum, Adam differences</li>
    <li><strong>CNN Dimensions</strong>: Calculate output sizes</li>
    <li><strong>LSTM Gates</strong>: Forget, Input, Output gates</li>
    <li><strong>Attention</strong>: Q, K, V computation</li>
    <li><strong>Regularization</strong>: When and why to use each technique</li>
  </ul>
</div>

<div class="alert alert-warning">
  <h4>‚ö†Ô∏è Common Exam Mistakes</h4>
  <ul>
    <li>Forgetting batch dimension in tensor operations</li>
    <li>Incorrect output size calculations for convolutions</li>
    <li>Not accounting for mask tokens in sequence models</li>
    <li>Confusing forward and backward passes</li>
    <li>Applying batch norm after activation (should be before)</li>
  </ul>
</div>

---

## üíº Real-World Applications

### Computer Vision
- Image classification, object detection, segmentation
- Self-driving cars, medical imaging, face recognition

### Natural Language Processing
- Machine translation, text generation, sentiment analysis
- Chatbots, information extraction, question answering

### Audio & Speech
- Speech recognition, music generation, speaker identification

### Recommendation Systems
- User-item interactions, collaborative filtering with deep networks

---

## üîó External Resources

### Courses
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)
- [Stanford CS231N - CNN for Visual Recognition](http://cs231n.stanford.edu/)
- [Stanford CS224N - NLP with RNNs](https://web.stanford.edu/class/cs224n/)

### Key Papers
- [Attention is All You Need - Transformer paper](https://arxiv.org/abs/1706.03762)
- [ImageNet Classification with Deep CNNs - AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
- [LSTM - Hochreiter & Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf)

### Libraries
- **PyTorch**: Flexible, pythonic framework
- **TensorFlow/Keras**: Production-ready, easier API
- **JAX**: Modern, functional approach
- **Hugging Face Transformers**: Pre-trained models for NLP

### Cheatsheets
- [PyTorch Cheatsheet](https://pytorch.org/tutorials/)
- [Keras API Reference](https://keras.io/)
- [CNN Architecture Visualization](https://github.com/vdumoulin/conv_arithmetic)

---

## üìû Need Help?

- Practice implementing architectures from scratch
- Experiment with pre-trained models on Hugging Face
- Review papers and implementation details
- Check the [Resources page]({{ site.baseurl }}/resources/) for more materials

**Last Updated**: December 2025
